{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 3: How to Load, Convert, and Write JSON Files in Python\n",
    "## DS 6001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "Make sure the virtual environment you created for this course is loaded as the notebooks kernel, and load the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "JSON and CSV are both text-based formats for the storage of data. It's possible to open either one in a plain text editor. Given this similarity, why does a CSV file usually take less memory than a JSON formatted file for the same data? Under what conditions could a JSON file be smaller in memory than a CSV file for the same data? [10 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CSV usually takes less memory than a JSON since CSVS primarily rely on positional orders of data fields whereas JSON relies on key-value pairs. This means that the repeated key names significantly increase the file size. Finally, CSVs only use commas/ line breaks to separate the records but JSOn uses braces, square brackets, and additional deliminaters. The extra characters add to the file size and memory footprint. A JSON file would be smaller in memory than a CSV file for data that is tree based since CSV only accomodates tabular data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "A [User-Agent header](https://en.wikipedia.org/wiki/User-Agent_header) (also called a user-agent string) is text that identifies the software you are using to access data from a web-server. It is considered to be good etiquette to write your user-agent string and send it to a web server along with your data request, and some web servers will enforce that by refusing to send you data unless you provide a user-agent string. \n",
    "\n",
    "Like a lot of things in software, there are stringent conventions that people follow when it comes to user-agent strings, and to operate in this world, you need to know how to abide by these conventions. \n",
    "\n",
    "One shortcut I recommend for writing a user-agent string in the correct format is to use an external tool. The website https://httpbin.org/user-agent examines the software you are using to access this website and mirrors back to you your user-agent string in the preferred format, in JSON format.\n",
    "\n",
    "### Part a\n",
    "Use your browser (Chrome, Firefox, Safari, etc.) to view the content at https://httpbin.org/user-agent. Copy the user-agent string and paste it here. Then identify the parts of this user-agent string that identify your web browser and your computer's operating system (Mac, Windows, etc.) [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "The last part of the string, \"Chrome/139\" identifies my web browser. The first set of information in the paranthesis (Macintosh; Intel Mac OS X 10_15_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "In part (a) you found your user-agent string for situations in which you use your web browser to access data on the web, but that user-agent string would be different if you access the same web server using Python. Use the `requests.get()` method to connect to https://httpbin.org/user-agent, then display the JSON formatted response. [6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user-agent': 'python-requests/2.32.5'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ag=requests.get(\"https://httpbin.org/user-agent\").json()\n",
    "user_ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Extract the user-agent string from the output in part (b). Then create a Python dictionary, save it as a Python variable named `myheaders`, and create a key inside this dictionary called `'User-Agent'` set equal to your user-agent string. (You will use `myheaders` in the rest of the problems in this assignment.) [6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'python-requests/2.32.5'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_agent_str = user_ag[\"user-agent\"]\n",
    "\n",
    "myheaders = {\"User-Agent\": user_agent_str}\n",
    "\n",
    "myheaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "There is a huge industry built around the central task of pulling data off of websites. Most of this work is automated using specially-built software. Some software has been explicitly identified and banned from accessing certain websites due to misuse of the data or creating too much traffic for the web server to handle.\n",
    "\n",
    "Websites often include a `robots.txt` file that lists the rules the web server asks users to follow, and publicizes the steps they've taken to restrict access. These files will sometimes call out specific user-agents. In the syntax of a `robots.txt` file, the phrase `Disallow: /` means \"disallow this user from accessing anything on this website.\"\n",
    "\n",
    "The following websites have `robots.txt` files that ban specific user-agents:\n",
    "\n",
    "* https://www.tiktok.com/robots.txt\n",
    "* https://www.espn.com/robots.txt\n",
    "* https://www.nytimes.com/robots.txt\n",
    "* https://www.amazon.com/robots.txt\n",
    "* https://www.cvilletomorrow.org/robots.txt (You should check out https://www.cvilletomorrow.org/, an awesome local online newspaper with award-winning data journalism)\n",
    "\n",
    "Choose one of these websites and find the specific user-agents that have been banned from access (user agents that have been named explicity, not `User-Agent: *` which applies to all user agents), and copy-and-paste that part of the `robots.txt` file here. Then do an internet search for one of these agents. Based on what you learn about the user-agent in question, why might you speculate that this user-agent was banned? (Any thoughtful answer to this part receives full credit). [6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 \n",
    "The NBA has saved data on all 30 teams' shooting statistics for the 2014-2015 season here: https://stats.nba.com/js/data/sportvu/2015/shootingTeamData.json. Take a moment and look at this JSON file in your web browser or with https://jsonhero.io. The structure of this particular JSON is complicated, but see if you can find the team-by-team data. In this problem our goal is to use `pd.json_normalize()` to get the data into a dataframe. The following questions will guide you towards this goal.\n",
    "\n",
    "### Part a\n",
    "Download the raw text of the NBA JSON file using `requests.get()`, and provide your user-agent to the NBA API by setting the `header` argument to the `myheaders` dictionary you created in problem 2. Then use `json.loads()` (or the `.json` attribute of the `requests` output) to parse the dictionaries and lists in the JSON formatted data. [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Based on your observations of the JSON structure, describe in words the path that leads to the team-by-team data. [8 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Use the `pd.json_normalize()` method to pull the team-by-team data into a dataframe. \n",
    "\n",
    "[Note: what makes this tricky is that one of the layers in the path to the data is named only `0`. This `0` is not a string like the other keys.  Specifying `0` in the `record_path`, either with or without quotes yields an error. There are two ways to solve this. The easiest way is to just skip over the `0` key entirely when writing down the `record_path`, and `pd.json_normalize()` will include the `0` layer on its own as a default behavior. The other way is to index the JSON data with both `['resultSets'][0]` first before passing the data to `pd.json_normalize()`, then writing the remaining layers in `record_path`]\n",
    "\n",
    "If you are successful, you will have a dataframe with 30 rows and 33 columns. The first row will refer to the Golden State Warriors, the second row will refer to the San Antonio Spurs, and the third row will refer to the Cleveland Cavaliers. The columns will only be named 0, 1, 2, ... at this point. [10 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "Find the path that leads to the headers (the column names), and extract these names as a list. Then set the `.columns` attribute of the dataframe you created in part c equal to this list. The result should be that the dataframe now has the correct column names.\n",
    "\n",
    "[Note: In this case, there's no need for `pd.json_normalize()` as the headers already exist as a list.]\n",
    "\n",
    "[8 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part e\n",
    "Save the NBA dataframe you extracted in problem 4 as a JSON-formatted text file on your local machine. Format the JSON so that it is organized as dictionary with three lists: `columns` lists the column names, `index` lists the row names, and `data` is a list-of-lists of data points, one list for each row. [Hint: this is possible with one line of code] [8 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "NASA has a pubic dataset of all asteroids that are in close proximity to Earth, searchable for any day, including today. The data contain the name of each asteroid, along with the absolute magnitude (a measure of luminosity), the minimum and maximum diameter in different units of measurement, the date and time of closest approach, the distance from Earth, and velocity of the astroid at the moment of its closest approach.\n",
    "\n",
    "To access the data for today, change the first line in the following code block to today's date in YYYY-MM-DD format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.nasa.gov/neo/rest/v1/feed?start_date=2025-05-28&end_date=2025-05-28&api_key=DEMO_KEY'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = '2025-05-28'\n",
    "url = f'https://api.nasa.gov/neo/rest/v1/feed?start_date={date}&end_date={date}&api_key=DEMO_KEY'\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: APIs are systems for exchanging data between servers and users on the internet. We will be discussing APIs in depth in module 4. The URL above specifies `api_key=DEMO_KEY`, which is for initially exploring APIs prior to signing up. The demo key has more restrictive limits on the amount of data a user can acquire than you can get by signing up for your own key. For module 4 we will work through the process of getting our own API keys and keeping them secret while writing Python code, but for this exercise using the demo key will work for what we need.)\n",
    "\n",
    "Use your web-browser or https://jsonhero.io to view the JSON data from this URL and determine the correct path that leads to the records we want to populate the rows of a dataframe.\n",
    "\n",
    "Then use `requests.get()`, `json.loads()`, and `pd.json_normalize()` to bring the data into Python and organize it in a dataframe. Use the `headers=myheaders` argument inside `requests.get()` to inform the NASA API about your user-agent string. [12 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "Pull data in JSON format from Reddit's top 25 posts on [/r/popular](https://www.reddit.com/r/popular/top/). Start by using `requests.get()` with the `headers=myheaders` argument on the website: http://www.reddit.com/r/popular/top.json. Then use `json.loads()` to parse the JSON format as python dictionaries and lists. If you look at the result, you should see JSON data. (If instead you see an access denied message, wait a couple minutes, restart your notebook and try again.)\n",
    "\n",
    "If you were to use `pd.json_normalize()` at this point, you would pull all of the features in the data into one dataframe, resulting in a dataframe with 172 columns. \n",
    "\n",
    "If we only wanted a few features, then looping across elements of the JSON list itself and extracting only the data we want may be a more efficient approach.\n",
    "\n",
    "Use looping - and not `pd.read_json()` or `pd.json_normalize()` - to create a dataframe with 25 rows (one for each of the top 25 posts), and only columns for `subreddit`, `title`, `ups`, and `created_utc`. You can follow the example listed in [section 3.3.3 of Surfing the Data Pipeline with Python](https://jkropko.github.io/surfing-the-data-pipeline/ch3.html#looping-across-records-to-extract-datapoints) as a way to start. (Your result might be a dataframe without any column headers. You can set the `.columns` attribute of the dataframe to the list `['subreddit', 'title', 'ups', 'created_utc']`.) [12 points]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds6001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
